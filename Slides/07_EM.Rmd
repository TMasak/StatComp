---
title: "Week 7: The EM-Algorithm"
subtitle: "MATH-517 Statistical Computation and Visualization"
author: "Tomas Masak"
# date: "`r format(Sys.time(), '%b %d, %Y')`"
date: "November 4th 2022"
output: beamer_presentation
classoption: "presentation"
theme: "Madrid"
colortheme: "seahorse"
footer: "Copyright (c) 2022, EPFL"
urlcolor: blue
header-includes:
  - \usepackage{bm}
  - \usepackage{stmaryrd}
  - \newcommand{\E}{\mathbb{E}}
  - \newcommand{\R}{\mathbb{R}}
  - \newcommand{\argmin}{\mathrm{arg\,min\;}}
  - \newcommand{\rank}{\mathrm{rank}}
  - \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Motivation From the Last Week

## CV for PCA Repaired

Assume that data $\mathbf{x}_n \in \R^p$ are i.i.d. realizations of $X \sim \mathcal{N}(\mu,\Sigma)$.

* split data into $K$ folds $J_1,\ldots,J_K$
* **for** $k=1,\ldots,K$
  - estimate $\mu$ and $\Sigma$ empirically using all but the $k$-th fold $J_k$, but truncate $\Sigma$ to be rank-$r$
  - **for** $n \in J_k$
    - split $\mathbf x_n$ a "missing" part $\mathbf{x}^{miss}$ that will be used for validation and an "observed" part $\mathbf{x}^{obs}$
    - predict $\mathbf{x}_n^{miss}$ from $\mathbf{x}_n^{obs}$ as discussed on the previous slide
  - **end for**
  - calculate $Err_k(r) = \sum_{n \in J_k} \| \mathbf x_n^{obs} - \widehat{\mathbf x}_n^{obs} \|_2^2$
* **end for**
* choose $\widehat{r} = \underset{r}{\argmin} \sum_{k=1}^K | J_k|^{-1} Err_k(r)$

## CV for PCA Repaired

```{r,fig.align='center',out.width="30%"}
library(lattice)
library(Matrix)
N <- 20
p <- 10
image(Matrix(array(rnorm(N*p),c(p,N))),scales=list(x=list("variables",cex=2),y=list(cex=2)), xlab=list("n=1,...,N", cex=2),
      ylab=list("i=1,...,p", cex=2))
```

```{r,fig.show='hold',out.width="30%"}
K <- 5
rowind <- matrix(sample(1:N),ncol=K)
k <- 1
  Mask <- array(1,c(p,N))
  Mask[,rowind[,k]] <- 0
  for(i in 1:length(rowind[,k])){
    colind <- sample(1:p,floor(p/2))
    Mask[colind,rowind[i,k]] <- 0.5
  }
  levelplot(t(Mask), col.regions = rev(gray(0:100/100)),scales=list(x=list(cex=2),y=list(cex=2)), xlab=list("n=1,...,N", cex=2),
      ylab=list("i=1,...,p", cex=2),main=list(label="1st fold", cex=2),colorkey=F)
plot(c(1,2,3),c(1,1,1), xlab="", ylab="", xaxt="n",yaxt="n",bty="n",pch=16,cex=2)
k <- 5
  Mask <- array(1,c(p,N))
  Mask[,rowind[,k]] <- 0
  for(i in 1:length(rowind[,k])){
    colind <- sample(1:p,floor(p/2))
    Mask[colind,rowind[i,k]] <- 0.5
  }
  levelplot(t(Mask), col.regions = rev(gray(0:100/100)),scales=list(x=list(cex=2),y=list(cex=2)), xlab=list("n=1,...,N", cex=2), ylab=list("i=1,...,p", cex=2),
      main=list(label="5th fold", cex=2),colorkey=F)
```

For every fold:

* use **black** entries to obtain $\widehat{\mu}$ and $\widehat{\Sigma}$
* predict **white** entries using **grey** entries and $\widehat{\mu}$ and $\widehat{\Sigma}$
* check the quality of your prediction

## Improvements?

* Grey entries provide information on $\mu$ and $\Sigma$, shouldn't we use it?
* Isn't it awkward to first split rows and then columns? Why not to just split the bivariate index set?

\bigskip
```{r,fig.show='hold',out.width="30%"}
Ind <- matrix(sample(1:(N*p)),ncol=K)
k <- 1
  Mask <- array(1,c(p,N))
  Mask[Ind[,k]] <- 0
  levelplot(t(Mask), col.regions = rev(gray(0:100/100)),scales=list(x=list(cex=2),y=list(cex=2)), xlab=list("n=1,...,N", cex=2),
      ylab=list("i=1,...,p", cex=2),main=list(label="1st fold", cex=2),colorkey=F)
plot(c(1,2,3),c(1,1,1), xlab="", ylab="", xaxt="n",yaxt="n",bty="n",pch=16,cex=2)
k <- 5
  Mask <- array(1,c(p,N))
  Mask[Ind[,k]] <- 0
  levelplot(t(Mask), col.regions = rev(gray(0:100/100)),scales=list(x=list(cex=2),y=list(cex=2)), xlab=list("n=1,...,N", cex=2),
      ylab=list("i=1,...,p", cex=2),main=list(label="5th fold", cex=2),colorkey=F)
```
\bigskip

\begin{exampleblock}{}
\centering To cope with this, we need to know how to do \textbf{MLE with missing data}.
\end{exampleblock}

# Expectation-Maximization (EM) Algorithm

## EM Algorithm

Iterative algorithm for calculating Maximum-Likelihood-Estimators (MLEs) in situations, where

* there is **missing data** complicating the calculations (Example 1 and 3 below) or
* it is beneficial to think of our data as if there were some components missing (Example 2 below)
  - when knowing that missing components would render the problem simple
  
We will assume that solving MLE with the **complete data** is simple.
  
EM will allow us to act like if we knew everything -- even when we don't or when we cannot use all the information.

## Notation and the Algorithm

* $\bm{X}_{obs}$ are the **observed** random variables
* $\bm{X}_{miss}$ are the **missing** random variables
* $\ell_{comp}(\theta)$ is the **complete** log-likelihood of $\bm{X} = (\bm{X}_{obs},\bm{X}_{miss})$
  - maximizing this to obtain MLE is supposed to be *simple*
  - $\theta$ denotes all the parameters, e.g. contains $\mu$ and $\Sigma$

\bigskip
\begin{exampleblock}{}
\centering Our task is to maximize $\ell_{obs}(\theta)$, the \textbf{observed} log-likelihood of $\bm{X}_{obs}$.
\end{exampleblock}
\bigskip

**EM Algorithm**: Start from an initial estimate $\widehat{\theta}^{(0)}$ and for $l=1,2,\ldots$ iterate the following two steps until convergence:

* **E-step**: calculate $\E_{\widehat{\theta}^{(l-1)}}\big[\ell_{comp}(\theta) \big| \bm{X}_{obs} = \mathbf{x}_{obs}\big] =: Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$
* **M-step**: optimize $\mathrm{arg\,max}_{\theta}\; Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$

## Ex.1: Censored Observations

Suppose you want to estimate the mean waiting time at an 
EPFL food truck:

* observed waiting times $\mathbf{x}_{obs} = (x_{obs}^{1}, \dots, x_{obs}^{N_{obs}})^\top$ for $\bm{X}_{obs}$.
* food truck closes when $N_{miss}$ individuals are still queuing, such that $\bm{X}_{miss} = (X_{miss}^{1}, \dots, X_{miss}^{N_{miss}})^\top$ are not observed but only a vector of right-censored waiting times $\tilde{\mathbf{x}}_{miss}$ with $\forall n: X_{miss}^{(n)} > \tilde{x}_{miss}^{(n)}$.
* overall $N = N_{obs} + N_{miss}$ individuals considered.

$\Rightarrow$ Apply EM-algorithm assuming weighting times iid.\ distributed \newline
follwing an exponential distribution with density $f(x) = \lambda \exp(-\lambda x)$.  

## Ex.1: Censored Observations -- E-step

* **E-step**: calculate $\E_{\widehat{\theta}^{(l-1)}}\big[\ell_{comp}(\theta) \big| \bm{X}_{obs} = \mathbf{x}_{obs}, \forall n: X_{miss}^{(n)} > \tilde{x}^{(n)}_{miss}\big] =: Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$

For iterations $l = 1, \dots$:
\vspace*{-10pt}

\begin{align*}
\E_{\widehat{\lambda}^{(l-1)}}\big[&\ell_{comp}(\theta) \mid \mathbf{x}_{obs}, \tilde{\mathbf{x}}_{miss} \big] =\\ &= 
  \E_{\widehat{\lambda}^{(l-1)}}\big[ \underbrace{N \log(\lambda) - \lambda \sum_{n=1}^{N_{obs}} X_{obs}^{(n)} - \lambda \sum_{n=1}^{N_{miss}} X_{miss}^{(n)}}_{
  \log (\prod_{n=1}^{N_{obs}} f(X_{obs}^{(n)}) \cdot \prod_{n=1}^{N_{miss}} f(X_{miss}^{(n)}) )
} \mid \mathbf{x}_{obs}, \tilde{\mathbf{x}}_{miss} \big]\\
  &= N \log(\lambda) - \lambda \sum_{n=1}^{N_{obs}} x_{obs}^{(n)} - \lambda \sum_{n=1}^{N_{miss}} \underbrace{\E_{\widehat{\lambda}^{(l-1)}}\big[X_{miss}^{(n)} \mid \tilde{\mathbf{x}}_{miss} \big]}_{\underset{"memoryless"}{\overset{X \sim Exponential(\hat{\lambda}^{(l-1)})}{=}} 1/\hat{\lambda}^{(l-1)} + \tilde{x}^{(n)}_{miss}}\\
  &= N \log(\lambda) - \lambda \Big( N_{obs} \bar{x}_{obs} + N_{miss} \frac{1}{\hat{\lambda}^{(l-1)}} + N_{miss} \bar{\tilde{x}}_{miss} \Big) = Q(\lambda,\hat{\lambda}^{(l-1)} )
\end{align*}

## Ex.1: Censored observations -- M-step
* **M-step**: optimize $\mathrm{arg\,max}_{\theta}\; Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$

\begin{align*}
Q(\lambda,\hat{\lambda}^{(l-1)} ) &= N \log(\lambda) - \lambda \Big( N_{obs} \bar{x}_{obs} + N_{miss} \frac{1}{\hat{\lambda}^{(l-1)}}   + N_{miss} \bar{\tilde{x}}_{miss}\Big)\\[10pt]
\Rightarrow \quad 
\frac{\partial Q}{\partial\lambda}(\lambda, \hat{\lambda}^{(l-1)}) &= \frac{N}{\lambda} - ( N_{obs} \bar{x}_{obs} + N_{miss} \frac{1}{\hat{\lambda}^{(l-1)}}  + N_{miss} \bar{\tilde{x}}_{miss} ) \overset{!}{=} 0 \\[10pt]
\Rightarrow \quad 
\frac{1}{\hat{\lambda}^{(l)}} &= \frac{ N_{obs} \bar{x}_{obs} + N_{miss} \frac{1}{\hat{\lambda}^{(l-1)}}  + N_{miss} \bar{\tilde{x}}_{miss} }{N}
\end{align*}

## Ex.2: Mixture distributions

One of the most popular applications of the EM-algorithm:\newline
Estimating mixture distributions for modelling multimodality

**Mixture of two Gaussian distributions**:

Let $X^{(1)}, \dots, X^{(N)}$ be iid.\ distributed as $X$ with probability density
$$
 f(x) = (1-\tau)\, \varphi\!\left(\frac{x-\mu_1}{\sigma_1}\right) + \tau\, \varphi\!\left(\frac{x-\mu_2}{\sigma_2}\right) 
$$
where

* $\varphi$ is the density of a standard normal, and  
* $\mu_1 < \mu_2$ and $\sigma_1^2, \sigma_2^2$ are the means and variances of the mixture components, 
and 
* $\tau \in (0,1)$ is the share of the second component
stacked in a vector $\boldsymbol{\theta} = (\tau, \mu_1,\mu_2,\sigma_1^2, \sigma_2^2)^\top$

## Ex.2: Mixture distributions -- factorization via latent variables

However, log-likelihood has no nice form:
$$
  \ell_{obs}(\boldsymbol{\theta}) = \sum_{n=1}^N \log \left( (1-\tau)\, \varphi\!\left(\frac{x-\mu_1}{\sigma_1}\right) \overset{{\color{red}\lightning}}{\boldsymbol{+}} \tau\, \varphi\!\left(\frac{x-\mu_2}{\sigma_2}\right) \right)
$$
**Trick**: add latent iid.\ component indicators $Z_n \sim Bernoulli(\tau)$ 
such that $X^{(n)} \mid Z^{(n)} = 0 \sim N(\mu_1, \sigma_1^2)$ and $X^{(n)} \mid Z^{(n)} = 1 \sim N(\mu_2, \sigma_2^2)$.

Given $Z^{(n)} = z^{(n)}$, $n=1, \dots, N$, the joint likelihood can be written as
$$
  L_{comp}(\boldsymbol{\theta}) = (1-\tau)^{N_1} \tau^{N_2} \prod_{n=1}^{N} \varphi\left( \frac{X^{(n)}-\mu_1}{\sigma_1}\right)^{(1-Z^{(n)})}  \varphi\left( \frac{X^{(n)}-\mu_2}{\sigma_2}\right)^{Z^{(n)}}
$$
with $N_2 = \sum_{n=1}^{N} Z^{(n)}$ and $N_1 = N - N_2$.

## Ex.2: Mixture distributions -- E-step -- Part I

* **E-step**: calculate $\E_{\widehat{\theta}^{(l-1)}}\big[\tilde{\ell}(\theta) \big| \bm{X} = \mathbf{x} \big] =: Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$

\begin{align*}
  \ell_{comp}(\boldsymbol{\theta}) &= \log L_{comp}(\boldsymbol{\theta}) = 
  N_1 \log(1 - \tau) + N_2 \log(\tau)\ +\\ 
  &+ \sum_{n=1}^{N} (1-Z^{(n)}) \log\varphi\left( \frac{X^{(n)}-\mu_1}{\sigma_1}\right) + \sum_{n=1}^{N} Z^{(m)} \log\varphi\left( \frac{X^{(n)}-\mu_2}{\sigma_2}\right)
\end{align*}
such that, we obtain
\begin{align*}
\E_{\widehat{\boldsymbol{\theta}}^{(l-1)}}&\big[\ell_{comp}(\boldsymbol{\theta}) \big| \bm{X} = \mathbf{x} \big] =
    \log(1-\tau) (N - \sum_{n=1}^{N} p^{(l-1)}_n) +  
    \log(\tau) \sum_{n=1}^{N} p^{(l-1)}_n +\\ 
    &+ \sum_{n=1}^{N} (1-p^{(l-1)}_n) \log\varphi\left( \frac{x^{(n)}-\mu_1}{\sigma_1} \right) + 
    \sum_{n=1}^{N} p^{(l-1)}_n \log\varphi\left( \frac{x^{(n)}-\mu_2}{\sigma_2}\right)
\end{align*}
with $p^{(l-1)}_n = \E_{\widehat{\theta}^{(l-1)}}\big[ Z^{(n)} \big| X^{(n)} = x^{(n)} \big] \overset{Bayes}{=} \frac{\varphi\left( \frac{x^{(n)}-\hat{\mu}_2^{(l-1)}}{\hat{\sigma}_2^{(l-1)}}\right) \hat{\tau}^{(l-1)}}{f_{\hat{\boldsymbol{\theta}}^{(l-1)}}(x^{(n)})}.$


## Ex.2: Mixture distributions -- M-step

* **M-step**: optimize $\mathrm{arg\,max}_{\theta}\; Q\big(\theta,\widehat{\theta}^{(l-1)}\big)$

Hence, $Q\big(\boldsymbol{\theta},\widehat{\boldsymbol{\theta}}^{(l-1)}\big)$ nicely splits into three parts

\begin{align*}
Q\big(&\boldsymbol{\theta},\widehat{\boldsymbol{\theta}}^{(l-1)}\big) =\\
    &\mathbf{A:}\quad \log(1-\tau) (N - \sum_{n=1}^{N} p^{(l-1)}_n) +  
    \log(\tau) \sum_{n=1}^{N} p^{(l-1)}_n +\\ 
    &\mathbf{B:}\quad + \sum_{n=1}^{N} (1-p^{(l-1)}_n) \log\varphi\left( \frac{x^{(n)}-\mu_1}{\sigma_1} \right) +\\ 
    &\mathbf{C:}\quad + \sum_{n=1}^{N} p^{(l-1)}_n \log\varphi\left( \frac{x^{(n)}-\mu_2}{\sigma_2}\right)
\end{align*}
which can be optimized separately, where $\mathbf{A}$ has the form of a binomial and $\mathbf{B}$ and $\mathbf{C}$ of (weighted) Gaussian log-likelihood 
$\Rightarrow$ optimize accordingly. 

## Ex.3: Multivariate Gaussian with Missing Entries

Let $\bm{X}^{(1)},\ldots,\bm{X}^{(N)}$ iid.\ $p$-variate normally distributed with mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$.

For each $n$, only a realization $\mathbf{x}^{(n)}_{obs}$ of a subvector $\bm{X}^{(n)}_{obs}$ of $\bm{X}^{(n)}$ is observed. 

The goal is to estimate $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ from the incomplete measurements. 

## Ex.3: Multivariate Gaussian with Missing Entries

Let further denote $\boldsymbol{\mu}^{(n)}_{obs}$ and $\boldsymbol{\Sigma}^{(n)}_{obs}$ the mean and the covariance of $\bm{X}^{(n)}_{obs}$, 
i.e. $\boldsymbol{\mu}^{(n)}_{obs}$ is just a sub-vector of $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}^{(n)}_{obs}$ is a sub-matrix of $\boldsymbol{\Sigma}$.

```{r, echo=F, fig.show="hold", out.width="45%"}
library(Matrix)
library(latex2exp)
set.seed(123)
ind_n <- sample(1:10,size=7)
mu <- rep(0,10)
mu[ind_n] <- 1             # which entries of mu correspond to observed
Sigma <- array(0,c(10,10))
Ind <- outer(mu==1,mu==1)  # bi-variate index set corresponding to observed entries
Sigma[Ind==1] <- 1
image((Matrix(mu)),main=list(TeX("$\\mu^{(n)}_{obs}$"), cex = 3), colorkey = FALSE, ylab = list("i=1,...,p", cex = 2), xlab = "")
image(Matrix(Sigma), main=list(TeX("$\\Sigma^{(n)}_{obs}$"), cex = 3), colorkey = FALSE, ylab = list("i=1,...,p", cex = 2), xlab = list("i=1,...,p", cex = 2))
```


## Ex.3: Multivariate Gaussian with Missing Entries

Recall the density $f(\mathbf{x})$ of a $p$-variate Gaussian:
\[
f(\mathbf{x}^{(n)}) \propto  \mathrm{det}(\boldsymbol{\Sigma})^{-\frac{1}{2}} \exp\left(- 
\frac{1}{2} \big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big)^\top \boldsymbol{\Sigma}^{-1} \big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big) \right),
\]

and, hence, log-likelihood are given by
\[
\begin{split}
\ell_{obs}(\boldsymbol{\mu},\boldsymbol{\Sigma}) &= \mathrm{const\,} - \frac{1}{2} \sum_{n=1}^N \mathrm{log\,det}(\boldsymbol{\Sigma}_{obs}^{(n)}) - \\
&\quad - \sum_{n=1}^N \frac{1}{2} \big( \mathbf{x}_{obs}^{(n)} - \boldsymbol{\mu}_{obs}^{(n)} \big) \big(\boldsymbol{\Sigma}_{obs}^{(n)}\big)^{-1} \big( \mathbf{x}_{obs}^{(n)} - \boldsymbol{\mu}_{obs}^{(n)} \big) \\
\ell_{comp}(\boldsymbol{\mu},\boldsymbol{\Sigma}) &= \mathrm{const\,} - \frac{N}{2} \mathrm{log\,det}(\boldsymbol{\Sigma}) - 
\sum_{n=1}^N \frac{1}{2} \underbrace{\big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big) \boldsymbol{\Sigma}^{-1} \big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big)}_{\mathrm{tr}\Big( \big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big) \big( \mathbf{x}^{(n)} - \boldsymbol{\mu} \big)^\top \boldsymbol{\Sigma}^{-1} \Big)}
.
\end{split}
\]

Optimizing $\ell_{comp}$ easier than optimizing $\ell_{obs}$.
$\Rightarrow$ EM-Algorithm.


## Ex.3: Multivariate Gaussian with Missing Entries -- E-step

* **E-step**: calculate $\E_{\widehat{\boldsymbol{\theta}}^{(l-1)}}\big[\ell_{comp}(\boldsymbol{\theta}) \big| \forall n: \bm{X}^{(n)}_{obs} = \mathbf{x}^{(n)}_{obs} \big] =: Q\big(\boldsymbol{\theta},\widehat{\boldsymbol{\theta}}^{(l-1)}\big)$ \newline
with $\boldsymbol{\theta} = (\boldsymbol{\mu}, \boldsymbol{\Sigma})$.

\begin{align*}
Q(\boldsymbol{\theta}|&\hat{\boldsymbol{\theta}}^{(l-1)}) = \mathrm{const\,} - \frac{N}{2} \mathrm{log\,det}(\boldsymbol{\Sigma}) -\\ 
&\quad - \sum_{n=1}^N \frac{1}{2}\mathrm{tr}\Big( \underbrace{
\E_{\boldsymbol{\theta}^{(l-1)}} \Big[ \big( \bm{X}^{(n)} - \boldsymbol{\mu} \big) \big( \bm{X}^{(n)} - \boldsymbol{\mu} \big)^\top \Big|  \forall n: \bm{X}^{(n)}_{obs} = \mathbf{x}^{(n)}_{obs} \Big]}_{
\overset{\text{some calculation}}{=} (\widehat{\mathbf x}^{(n)(l-1)}-\boldsymbol{\mu})(\widehat{\mathbf x}^{(n)(l-1)}-\boldsymbol{\mu})^\top + \mathbf{C}^{(n)}
} \boldsymbol{\Sigma}^{-1} \Big)
\end{align*}

with $\hat{\mathbf{x}}^{(n)(l-1)} = \E_{\widehat{\boldsymbol{\theta}}^{(l-1)}}\big[ \bm{X}^{(n)} \big| \forall n: \bm{X}^{(n)}_{obs} = \mathbf{x}^{(n)}_{obs} \big]$ and \newline
$\mathbf{C}^{(n)} = \left\{ \mathrm{Cov}_{\widehat{\boldsymbol{\theta}}^{(l-1)}}\left(X^{(n)}_i, X^{(n)}_j \mid \bm{X}^{(n)}_{obs} = \mathbf{x}^{(n)}_{obs} \right) \right\}_{i,j}$.

## Ex.3: Multivariate Gaussian with Missing Entries -- M-step

* **M-step**: optimize $\mathrm{arg\,max}_{\boldsymbol{\theta}}\; Q\big(\boldsymbol{\theta},\widehat{\boldsymbol{\theta}}^{(l-1)}\big)$

\begin{align*}
Q\big(\boldsymbol{\theta},&\widehat{\boldsymbol{\theta}}^{(l-1)}\big) = \mathrm{const\,} - \frac{N}{2} \mathrm{log\,det}(\boldsymbol{\Sigma}) - \\
&- \sum_{n=1}^N \frac{1}{2}\mathrm{tr}\Big( (\widehat{\mathbf x}^{(n)(l-1)}-\mu)(\widehat{\mathbf x}^{(n)(l-1)}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} \Big) - \frac{1}{2} \mathrm{tr}\big( \mathbf{C} \boldsymbol{\Sigma}^{-1} \big),
\end{align*}

has a similar form as a multivariate normal and 
estimators can be derived accordingly, resulting in
\[
\hat{\boldsymbol{\mu}}^{(l)} = N^{-1} \sum_{n=1}^{N} \widehat{\mathbf x}^{(n)(l-1)}
\]

and 
\[
\widehat{\boldsymbol{\Sigma}}^{(l)} = \frac{1}{N} \sum_{n=1}^N \big[ (\widehat{\mathbf x}^{(n)(l-1)}-\boldsymbol{\mu})(\widehat{\mathbf x}^{(n)(l-1)}-\boldsymbol{\mu})^\top + \mathbf{C}^{(n)} \big].
\]

## Recap

\small
Example 1:

* part of data missing but their censored versions carry some information  
* the likelihood is linear (w.r.t. observations) and thus the **E-step** coincides with imputation (missing data replaced by their expectations)
  - this is rare!
  
Example 2:

* there is no true missing data here, but it is beneficial to imagine it
* the likelihood is linear w.r.t. the imagined observations $\Rightarrow$ simplification

Example 3:

* likelihood of observed data easy to formulate, yet harder to optimize directly
* no linearity in log-likelihood $\Rightarrow$ no imputation, more effort to compute expected likelihood

## References

* Dempster, A. P., N. M. Laird \& D. B. Rubin. (1977) "Maximum likelihood from incomplete data via the EM algorithm." *Journal of the Royal Statistical Society: Series B (Methodological)* 39.1: 1-22.
    - one of the most cited papers in statistics of all time

* Little, R. J., \& Rubin, D. B. (2019). *Statistical analysis with missing data*. 3rd Edition.

## Assignment 5 [5 %]

Simulate data (using Manual 9) from the mixture of two Gaussian distributions and implement the EM algorithm from Example 2 above. 
Use absolute change in $\ell$ of observed data as convergence criterion.

The following should naturally be done, though it is not mandatory:

* Visualize the resulting parametric density estimators.
* Try running your algorithm from different starting points.
  - How sensitive is your algorithm to your choice of starting point?
  - Can you find a bad starting point where your algorithm fails?

## Project Ideas

\small
* Cross-validation for PCA
  - A simulation study to compare the advantages of EM compared to what we did last week.
  - Two more approaches to CV for PCA to be considered and compared.
* Comparison of local regression implementations in different R packages.
  - [Wickham (2011)](http://vita.had.co.nz/papers/density-estimation.html) examines several packages for KDE in R. Not only there are huge differences in terms of speed, but some of the packages are even inconsistent! Makes one wonder what is the situation with local regression.
* Various simulation studies on bandwidth selection in KDE or local linear regression.
* Comparison of variable selectors in regression.
  - [Hastie et al. (2020)](https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.short) have some surprising results in their simulation study, but one important method (adaptive lasso) is omitted. Try to recreate the study with adaptive lasso included.

